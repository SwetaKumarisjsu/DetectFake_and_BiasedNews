{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "from gensim import utils\n",
    "\n",
    "#Adding this to filter all the warning messages\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def dataClean(textdata):\n",
    "    \"\"\"\n",
    "    Removing special characters\n",
    "    Removing empty cells\n",
    "    Removing stopwords\n",
    "    \"\"\"\n",
    "    textdata = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", textdata)\n",
    "    textdata = textdata.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    textdata = [w for w in textdata if not w in stops]\n",
    "    textdata = \" \".join(textdata)\n",
    "    return (textdata)\n",
    "\n",
    "\n",
    "def dataCleanup(textdata):\n",
    "    textdata = dataClean(textdata)\n",
    "    textdata = textdata.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return textdata\n",
    "\n",
    "\n",
    "def labeledSentences(data):\n",
    "    sentences = []\n",
    "    for index, row in data.iteritems():\n",
    "        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doc2Vector(path,vector_dimension=300):\n",
    "    \"\"\"\n",
    "    Generating training and testing data using Doc2Vec\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    missing_rows = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i, 'text'] != data.loc[i, 'text']:\n",
    "            missing_rows.append(i)\n",
    "            \n",
    "    data = data.drop(missing_rows).reset_index().drop(['index','id'],axis=1)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data.loc[i, 'text'] = dataCleanup(data.loc[i,'text'])\n",
    "\n",
    "    x = labeledSentences(data['text'])\n",
    "    y = data['label'].values\n",
    "\n",
    "    text_model = Doc2Vec(min_count=1, window=5, vector_size=vector_dimension, sample=1e-4, negative=5, workers=7, epochs=10,\n",
    "                         seed=1)\n",
    "    text_model.build_vocab(x)\n",
    "    text_model.train(x, total_examples=text_model.corpus_count, epochs=text_model.iter)\n",
    "\n",
    "    train_size = int(0.8 * len(x))\n",
    "    test_size = len(x) - train_size\n",
    "\n",
    "    text_train_arrays = np.zeros((train_size, vector_dimension))\n",
    "    text_test_arrays = np.zeros((test_size, vector_dimension))\n",
    "    train_labels = np.zeros(train_size)\n",
    "    test_labels = np.zeros(test_size)\n",
    "\n",
    "    for i in range(train_size):\n",
    "        text_train_arrays[i] = text_model.docvecs['Text_' + str(i)]\n",
    "        train_labels[i] = y[i]\n",
    "\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(train_size, train_size + test_size):\n",
    "        text_test_arrays[j] = text_model.docvecs['Text_' + str(i)]\n",
    "        test_labels[j] = y[i]\n",
    "        j = j + 1\n",
    "\n",
    "    return text_train_arrays, text_test_arrays, train_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data():\n",
    "    \"\"\"\n",
    "    Generating processed string\n",
    "    \"\"\"\n",
    "    path = 'data/train.csv'\n",
    "    vector_dimension=300\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    missing_rows = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i, 'text'] != data.loc[i, 'text']:\n",
    "            missing_rows.append(i)\n",
    "            \n",
    "    data = data.drop(missing_rows).reset_index().drop(['index','id'],axis=1)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data.loc[i, 'text'] = dataCleanup(data.loc[i,'text'])\n",
    "\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    x = data.loc[:,'text'].values\n",
    "    y = data.loc[:,'label'].values\n",
    "    print(x,y)\n",
    "\n",
    "    train_size = int(0.8 * len(y))\n",
    "    test_size = len(x) - train_size\n",
    "    \n",
    "    print(\"train_size: \",train_size)\n",
    "    print(\"test_size\", test_size)\n",
    "\n",
    "    xtrain = x[:train_size]\n",
    "    xtest = x[train_size:]\n",
    "    ytrain = y[:train_size]\n",
    "    ytest = y[train_size:]\n",
    "\n",
    "    np.save('x_training_data_shuffled.npy',xtrain)\n",
    "    np.save('x_testing_data_shuffled.npy',xtest)\n",
    "    np.save('y_training_data_shuffled.npy',ytrain)\n",
    "    np.save('y_testing_data_shuffled.npy',ytest)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
